{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ74iiCr3hKB"
      },
      "source": [
        "**Reddit** sentiment analysis and prediction using pyspark **REGRESSION MODEL**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oXP8rfJs0GdU"
      },
      "outputs": [],
      "source": [
        "from IPython import display\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark import SparkContext\n",
        "\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR4zqEWd0cY_",
        "outputId": "13f06353-2ebc-48fd-d450-27d9fda3a1d1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "21/12/06 10:32:40 WARN Utils: Your hostname, pop-os resolves to a loopback address: 127.0.1.1; using 192.168.29.178 instead (on interface wlo1)\n",
            "21/12/06 10:32:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/codevardhan/dev/mental_health_analysis/env/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "21/12/06 10:32:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "/home/codevardhan/dev/mental_health_analysis/env/lib/python3.9/site-packages/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "sc =SparkContext()\n",
        "sqlContext = SQLContext(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FEcbvAku0dX_"
      },
      "outputs": [],
      "source": [
        "customSchema = StructType([\n",
        "    StructField(\"clean_text\", StringType()), \n",
        "    StructField(\"category\", StringType())])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeKEJfA90eVd",
        "outputId": "daeb7e64-8ce4-4075-9fdd-04fc578a8c81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/BDMA\n",
            "Reddit_Uncleaned.csv  redt_dataset.csv\tTweets_Uncleaned.csv\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/BDMA/\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xikE9CTA1BNx",
        "outputId": "a48f24a3-d08a-42a4-ffa5-9ec9bdbcb7f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/BDMA\n",
            "+--------------------+--------+\n",
            "|          clean_text|category|\n",
            "+--------------------+--------+\n",
            "| understand that ...|       1|\n",
            "|welcome depressio...|       1|\n",
            "| don’ want kill m...|       1|\n",
            "|’ been very depre...|       1|\n",
            "|everything seemed...|       1|\n",
            "| just really need...|      -1|\n",
            "|but instead devel...|       1|\n",
            "|you just laughed ...|       1|\n",
            "| really hope heav...|      -1|\n",
            "| lost grandfather...|      -1|\n",
            "| have zero motiva...|      -1|\n",
            "| mental health ha...|      -1|\n",
            "|that’ that’ the p...|       0|\n",
            "| want die not rea...|      -1|\n",
            "| feel like depres...|      -1|\n",
            "| can afford shit ...|      -1|\n",
            "| let people know ...|       1|\n",
            "| fucking sick and...|      -1|\n",
            "| cant cant out fo...|       1|\n",
            "|nowadays feel lik...|      -1|\n",
            "+--------------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pwd\n",
        "filename = '/content/drive/MyDrive/BDMA//redt_dataset.csv'\n",
        "df = sqlContext.read.format(\"csv\").option(\"header\", \"true\").schema(customSchema).load(filename)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUJeMuBk3a5H",
        "outputId": "750d5b9c-2b35-44f1-eb82-bb71ef149da4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------+\n",
            "|          clean_text|category|\n",
            "+--------------------+--------+\n",
            "| understand that ...|       1|\n",
            "|welcome depressio...|       1|\n",
            "| don’ want kill m...|       1|\n",
            "|’ been very depre...|       1|\n",
            "|everything seemed...|       1|\n",
            "+--------------------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = df.na.drop(how='any')\n",
        "data.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSMvQp321S7_",
        "outputId": "d59fdb59-68c1-4252-ef4b-728fa8f8d594"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- clean_text: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qAqMubD3UgE",
        "outputId": "a8160e3c-36f9-457d-a76c-26fb4e8aad66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+-----+\n",
            "|category|count|\n",
            "+--------+-----+\n",
            "|      -1|  470|\n",
            "|       1|  459|\n",
            "|       0|   43|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "data.groupBy(\"category\").count().orderBy(col(\"count\").desc()).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6fzrgrb3dyH"
      },
      "source": [
        "*Model Pipeline*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "UGp7MuJJ34Na"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# regular expression tokenizer\n",
        "regexTokenizer = RegexTokenizer(inputCol=\"clean_text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "\n",
        "# stop words\n",
        "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] \n",
        "\n",
        "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
        "\n",
        "# bag of words count\n",
        "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=15000, minDF=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3X622ihI4x8a",
        "outputId": "15a08f4b-1f23-4031-af6e-95aa328805e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
            "|          clean_text|category|               words|            filtered|            features|label|\n",
            "+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
            "| understand that ...|       1|[understand, that...|[understand, that...|(1890,[0,1,2,3,4,...|  1.0|\n",
            "|welcome depressio...|       1|[welcome, depress...|[welcome, depress...|(1890,[0,1,2,3,4,...|  1.0|\n",
            "| don’ want kill m...|       1|[don, want, kill,...|[don, want, kill,...|(1890,[0,3,4,5,9,...|  1.0|\n",
            "|’ been very depre...|       1|[been, very, depr...|[been, very, depr...|(1890,[0,1,2,3,5,...|  1.0|\n",
            "|everything seemed...|       1|[everything, seem...|[everything, seem...|(1890,[0,1,2,3,4,...|  1.0|\n",
            "+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
        "label_stringIdx = StringIndexer(inputCol = \"category\", outputCol = \"label\")\n",
        "\n",
        "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
        "\n",
        "# Fit the pipeline to training documents.\n",
        "pipelineFit = pipeline.fit(data)\n",
        "dataset = pipelineFit.transform(data)\n",
        "dataset.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_-y71M941hc"
      },
      "source": [
        "*Partition Training & Test sets / Model Training and Evaluation*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAAqz0Tn4zAI",
        "outputId": "4ca394e2-1218-486e-df9a-381194f637e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset Count: 684\n",
            "Test Dataset Count: 288\n"
          ]
        }
      ],
      "source": [
        "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
        "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
        "print(\"Test Dataset Count: \" + str(testData.count()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V01zYqHK458t",
        "outputId": "7550d059-8115-40ed-e511-0178f8d1efe5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "|                    clean_text|category|                   probability|label|prediction|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "| can live with the guilt an...|      -1|[0.9762245509655013,0.02121...|  0.0|       0.0|\n",
            "|first sorry for bad english...|      -1|[0.9521293975302003,0.04348...|  0.0|       0.0|\n",
            "| finally starting recover f...|      -1|[0.9472628630783014,0.04669...|  0.0|       0.0|\n",
            "|writing this after another ...|       1|[0.9454040410045139,0.05121...|  1.0|       0.0|\n",
            "| been poor whole life finis...|      -1|[0.9303841948252422,0.04527...|  0.0|       0.0|\n",
            "|this reddit post asking for...|      -1|[0.9193142814358951,0.07917...|  0.0|       0.0|\n",
            "|’ tired don’ complain becau...|       1|[0.9031143454381103,0.08881...|  1.0|       0.0|\n",
            "| feel held hostage unfortun...|      -1|[0.8894822482777025,0.10991...|  0.0|       0.0|\n",
            "| all long time lurker first...|      -1|[0.8842477115693349,0.10303...|  0.0|       0.0|\n",
            "|for years have been struggl...|       1|[0.8691871849895906,0.11744...|  1.0|       0.0|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
        "lrModel = lr.fit(trainingData)\n",
        "\n",
        "predictions = lrModel.transform(testData)\n",
        "\n",
        "predictions.filter(predictions['prediction'] == 0).select(\"clean_text\",\"category\",\"probability\",\"label\",\"prediction\")\\\n",
        ".orderBy(\"probability\", ascending=False).show(n = 10, truncate = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKWDK0Eg47Iu",
        "outputId": "d009cd2b-0742-43bb-e4de-20ce931ad217"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6338999788762146"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRjXjFXZ492E"
      },
      "source": [
        "*Logistic Regression using TF-IDF Features*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2POd3t248Y4",
        "outputId": "26217233-e712-45d7-aeb6-13e29acc682e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "|                    clean_text|category|                   probability|label|prediction|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "| can live with the guilt an...|      -1|[0.9883120311720182,0.00988...|  0.0|       0.0|\n",
            "|first sorry for bad english...|      -1|[0.9522443627753939,0.04354...|  0.0|       0.0|\n",
            "| all long time lurker first...|      -1|[0.9462319038715808,0.04594...|  0.0|       0.0|\n",
            "| been poor whole life finis...|      -1|[0.9365868334925681,0.04368...|  0.0|       0.0|\n",
            "| been stuck tearing for hou...|      -1|[0.9365348578680442,0.04847...|  0.0|       0.0|\n",
            "|writing this after another ...|       1|[0.9358110650866001,0.05933...|  1.0|       0.0|\n",
            "| finally starting recover f...|      -1|[0.9353617910829934,0.05799...|  0.0|       0.0|\n",
            "| have been unemployed for q...|       1|[0.9092383972026635,0.08695...|  1.0|       0.0|\n",
            "| swear the past could actua...|      -1|[0.890515628693977,0.080403...|  0.0|       0.0|\n",
            "| can feel myself sinking ag...|       1|[0.8728293166816685,0.11809...|  1.0|       0.0|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
        "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])\n",
        "\n",
        "pipelineFit = pipeline.fit(data)\n",
        "dataset = pipelineFit.transform(data)\n",
        "\n",
        "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
        "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
        "lrModel = lr.fit(trainingData)\n",
        "\n",
        "predictions = lrModel.transform(testData)\n",
        "\n",
        "predictions.filter(predictions['prediction'] == 0) \\\n",
        "    .select(\"clean_text\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
        "    .orderBy(\"probability\", ascending=False) \\\n",
        "    .show(n = 10, truncate = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71ewABfo5BX2",
        "outputId": "db9c281f-0672-4be2-ead5-ac3410febb49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.613804959930691"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0INv75c5Eat"
      },
      "source": [
        "*Cross-Validation*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "jnUUdYzb5CfL"
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
        "\n",
        "pipelineFit = pipeline.fit(data)\n",
        "dataset = pipelineFit.transform(data)\n",
        "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
        "\n",
        "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tw-qMkXp5GGV"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "# Create ParamGrid for Cross Validation\n",
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
        "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
        "#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n",
        "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
        "             .build())\n",
        "\n",
        "# Create 5-fold CrossValidator\n",
        "cv = CrossValidator(estimator=lr, \\\n",
        "                    estimatorParamMaps=paramGrid, \\\n",
        "                    evaluator=evaluator, \\\n",
        "                    numFolds=5)\n",
        "\n",
        "cvModel = cv.fit(trainingData)\n",
        "\n",
        "predictions = cvModel.transform(testData)\n",
        "# Evaluate best model\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KD37oAzz5HRR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Log_Reg_Reddit.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
